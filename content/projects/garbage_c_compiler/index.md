---
title: "Garbage C Compiler (gcc)"
description: "c-90 compiler written with c++ flex and bison"
link: https://github.com/langproc/langproc-2023-cw-garbage_c_compiler_gcc
screenshot: featured.png
date: '2024-03-19'
layout: 'portfolio'
draft: false
---
I worked in a team of 2 to create a c90 to risc-v compiler. it used flex and bison to generate the lexer and parser, generating a c++ abstract syntax tree. we managed to implement a large subset of the source language over the 4 weeks we had to complete our compiler, ranging from all of the major operators for int float and double data types, strings, arrays and all of c90's control flow statements. the project was a resounding success, with our compiler passing over 90% of the seen test-cases, one of the highest pass rates in the cohort.  
*(sadly I will not share my source code in the interest of upholding academic integrity by not posting solutions to an ongoing piece of coursework)*

## that's all well and good, but how exactly does a compiler work (skip this if you know how a compiler works)?

a compiler is nothing more than a piece of software that takes a program written in one language as an input, and generates a program in another language as an output. this means that compilers need to first interpret the source file. compilers do this by first splitting up the source code into a stream of tokens (the basic block of the language eg: int, =, [), a process called [lexing](https://en.wikipedia.org/wiki/lexical_analysis). these tokens are then converted into a syntactic analyzer, which group tokens together using grammar rules to generate a tree form representation of the source code. several [optimization techniques](https://en.wikipedia.org/wiki/optimizing_compiler#specific_techniques) are then applied, the most complex and computationally expensive part of most modern compilers. the tree optimized tree is finally traversed to generate code in the target language.

## implementation: the parser

lexical and syntactic analysis are quite mature fields, and implementing a good parser can be quite complicated process, so we opted to use a preexisting tool to to generate the parser for us. to generate our lexer we used [flex](https://ftp.gnu.org/old-gnu/manuals/flex-2.5.4/html_mono/flex.html). our flex file mostly contains a list of rules for what tokens to return for given regular expressions, for example:

```lex
// definitaion section
d   [0-9]
l   [a-za-z_]
%% // rules section
"int"       {return(int);} // if mathcing "int", return an int token
{l}({l}|{d})*     {yylval.string = new std::string(yytext); return(identifier);} // a lowercase letter followed by anny combination of letters or numbers returns an identifier with yyvval.string being set to the string value of the identifier.
%% // user code section
```

flex generates a c++ file that externs the function yylex(), which parses until matching a rule, after which it runs our actions. in the case of our identifier, it returns sets yylval and then escapes yylex(), returning the token type. each time yylex() is called, it picks off again where it left off, allowing us to iterate through the tokens in the source file. flex achieves this by using a [dfa](https://en.wikipedia.org/wiki/deterministic_finite_automaton) to decide how to match tokens.  
the next step is to organize these tokens into a tree resembling the structure of the source code. to do this we used [bison](https://www.gnu.org/software/bison/manual/html_node/introductior.html); a program that generates a parser from a grammar defined in our .y file *(as a side note, the reason why bison uses the .y file extension is because bison is a free and open source version of a similar program called yacc)*, in a similar structure to our flex file. the bison utility creates a bison parser implementation file that implements a function called yyparse(), that parses tokens generated by calling yylex. the example below shows how struct declarations are defined:

```yacc
// declaration section
%token mul_assign div_assign mod_assign add_assign //etc...
%type <node> conditional_expression assignment_expression assignment_operator //etc...
%% // rules section
// an assignment expression is either a conditional expression, or a uniary expression followed by an assignment operator followed by an assignment expression.
assignment_expression
// upon a gramar rule being matched, the rendered (what the template gets turned into by bison) version of the c++ template in {} brackets at the end of each case is called.
// $1, $2 ...etc represent the $$ generated by the rule (or raw value in the case of terminal components) matching the nth component.
 : conditional_expression // implicit {$$ = $1;}
 | unary_expression assignment_operator assignment_expression {$$ = new assignment(($1), $2, $3); }
 ;

assignment_operator
 : '='    { $$ = assign_op_equals; }
 | mul_assign { $$ = mul_op_assign; }
 | div_assign { $$ = div_op_assign; }
 | mod_assign { $$ = mod_op_assign; }
 | add_assign { $$ = add_op_assign; }
 ;

%% // user code section
```

as well as defining the grammar, the .y file also contains some other key components, such as the root node declaration, the definition of our parse function, and declarations for all of the variants of the ast node typed (also used to generate yylval).

## implementation: code generation

now that we have generated an [ast](https://en.wikipedia.org/wiki/abstract_syntax_tree), the next step would be to implement a series of optimizations. this is a complicated process and often involves converting the source code into an intermediate representation (ir) for further analysis. one of the most common ways of doing this is to emit an ir like the llvm ir which can then be optimized and compiled to any instruction set supported by llvm. seeming as the goal of this project was to generate a simple compiler, we skipped the optimization steps and simply generated our risc-v assembly from our ast.

the source code for our ast is defined over several files, with each node inheriting from a basic node, and implementing a few basic functions, namely a constructor, destructor, debug print and most importantly emitrisc, which generates the assembly code. take for example a for loop:

```cpp

void for::emitrisc(std::ostream &stream, context &context) const {  
    std::string address = std::to_string((unsigned long long)(void **)this); // generate a unique label identifier - each node in the ast must be at a unique address

    context.push_continue("endloop" + address);     // push labels to jump to for break and continue for the body to the global context
    context.push_break("post_expr" + address);

    expression_init_->emitrisc(stream, context);    // generate init statement (eg: int i)
    stream << "j "
           << "check" << address << "\n";           // jump to checking the condition
    stream << "body" << address << ":\n";           // label for where the body of the for loop begins

    statement_->emitrisc(stream, context);          // generate code for the body

    stream << "post_expr" << address << ":\n";      // label for post expression (eg: i++)
    if (expression_post_ != nullptr) {              // generate post expression if exists
        expression_post_->emitrisc(stream, context);
    }
    stream << "check" << address << ":\n";          // label for condition (eg: i < 10)
    expression_check_->emitrisc(stream, context);   // generate condition logic
    stream << "bnez a0, "
           << "body" << address << "\n";            // output of condition logic is stored in a0, so branch based on result.
    stream << "endloop" << address << ":\n";        // endloop label for continue statement

    context.pop_continue();                         // pop labels for jump and continue
    context.pop_break();
}
```

as we can see, this calls emitrisc for all the necessary child nodes for each node of the ast, effectively traversing the tree and emitting code as it goes. some nodes are more complex, requiring a few extra functions, for example identifers need to behave differently based on if they are being assigned to or read from.  
